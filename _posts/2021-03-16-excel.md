LA FEUILLE QUI CALCULE LE RÉEL


Benjamin Efrati, Vincent Rioux
Recursion Lab



[visuel 1 : computer-cult]

Le tableur informatisé a été inventé en Californie à la fin des années 1970, alors que les premiers ordinateurs personnels entrent sur le marché. Il se présente sous la forme d'une matrice de cellules organisées en lignes et colonnes, permettant l’affichage et la “manipulation” de nombres entiers et flottants, mais aussi chaînes de caractères alphanumériques. En août 2020, un comité de scientifiques entreprend de réformer les acronymes utilisés pour se référer à certains gènes humains. Ici, l’importance d’outils permettant l’automatisation de tâches bureaucratiques semble interférer avec des enjeux de taxonomie phylogénétique. Ce débat sera le point de départ d’une réflexion sur le sens contemporain du terme “technocratie”. 

L’une des raisons avancées pour réformer le système taxonomique en génétique renvoie directement à la syntaxe de Microsoft Excel : certains de ces acronymes comme MARCH2 ou encore SEPT1 sont automatiquement convertis en dates par ce logiciel de comptabilité, induisant des erreurs dans de nombreux documents de travail et publications. Obtenir la modification de l’algorithme d’autocorrection des saisies paraît cependant illusoire face au nombre d’utilisateurs qui était estimé à 750 millions entre 2016 et 2020. Cette faillibilité de l’outil de Microsoft, loin d’être isolée, ne fait que souligner l'autorité de la multinationale sur l’économie de la connaissance. Dans ce contexte, est-il possible de départager les enjeux épistémologiques, politiques et économiques malgré le monopole de Microsoft sur le marché des outils bureautiques, et ce dans les administrations publiques et privées? L’actualité inattendue de la feuille de calcul dans le domaine de la génétique nous permettra donc d’initier une réflexion sur les conditions de possibilité du travail informatisé. Cette réflexion partira donc d’outils destinés aux domaines tels que la comptabilité et la logistique, afin d’interroger la dimension technocratique du conditionnement méthodologique dans la recherche scientifique ainsi que dans les industries culturelles.

Nous tâcherons donc de prime abord de repérer la place et la position dévolues au tableur dans nos sociétés globalisées. Il s’agira ensuite de déterminer ce que la feuille de calcul en tant qu'épiphénomène des sciences de l’informatique nous apprend sur le conditionnement des régimes de représentation en ce qui concerne l'écologie, la neurodiversité, l'évolution et aux réseaux de télécommunication. La réflexion sur l’impact à venir de l'intelligence artificielle s’impose au devant de la scène médiatique, et les puissances politiques internationales semblent déterminées à mettre en place des espaces de réflexion commun à ce sujet. Il est donc plus que jamais temps d’interroger les tableurs: ils dessinent les contours de notre quotidien tout autant que les structures algorithmiques qui donnent forme aux données numériques produites en masse par une humanité toujours plus globalisée. 

Nous partons intentionnellement d’un problème qui concerne à la fois la génétique et les technologies numériques à dessein. Notre hypothèse de travail est que les normes en vigueur dans la mise en forme de représentations abstraites dans secteur d’activité du logiciel ont un impact sur les imaginaires et les pratiques qui découlent de leur utilisation à grande échelle: il existerait une corrélation entre les principes de fonctionnement des outils logiciels et les représentations idéologiques de leurs utilisateurs. Un tel impact pourrait en outre être observé à différentes échelles: au niveau de la cognition, au niveau comportemental, au niveau social, au niveau financier et enfin au niveau des instances de régulation des modes d’existence des individus, à savoir la gouvernance politique. L’existence d’une corrélation entre conception logicielle et idéologie contribuerait donc à rendre compte des transformations du système politique au cours des cinquante dernières années. 

Il importe de souligner que cet article vise avant tout à expliciter l’idée que les normes implicites à l’œuvre dans les environnements de travail numérique ont la prétention de s’inscrire dans la thématique de l’évolution. Il nous incombe donc d’évaluer ultérieurement dans le détail la validité de cette corrélation apparente entre d’une part les présupposés et les pratiques en vigueur dans le domaine de la conception de logiciels, et d’autre part l’efficacité politique des représentations idéologiques qui prolifèrent dans l’imagerie technologique. Loin de proposer une analyse causale des rapports entre les environnements de travail informatique et les transformations des représentations populaires à leur sujet, nous nous contenterons donc ici d’esquisser le plan dans lequel s’inscrit le débat en question.

1-Grilles de lecture


[visuel 2 : visicalc]

Comptabilité en extase
À première vue, les outils logiciels les plus génériques se présentent sous la forme d’une transposition de l’environnement bureautique dans un espace virtuel: les documents textuels, les tableurs, et dans une autre mesure les navigateurs font référence au monde physique. On pense notamment à l’assistant virtuel de la suite Microsoft Office, un trombone nommé Clippy destiné à prodiguer des conseils aux utilisateurs. Dans le cas des tableurs, ils se réfèrent aux livres de comptabilité, dont les limitations physiques contraignaient les comptables et les obligeaient à faire preuve d’ingénuité pour représenter leurs données de manière synthétique. Un tableur exprime en deux dimensions les éléments qu’il prétend organiser. Sous la forme d’une feuille de calcul, on peut aussi bien représenter un plan de travail, un agenda, une liste de tâches ou de personnes, c’est à dire des documents qui ne font pas appel à des fonctions mathématiques. L’émergence puis la généralisation de ce type de représentations bidimensionnelles indique que le tableur est devenu un système de notation pour ainsi dire intuitif. Si l’on s’accorde à dire qu’Excel permet de formuler des modélisations simplifiées de phénomènes complexes, ces dernières sont rendues possibles par une simplification normalisée des phénomènes représentés. Pour comprendre dans quelle mesure cet outil permet de penser une forme de continuité entre la comptabilité et le développement de l’intelligence artificielle, revenons à une description du fonctionnement du tableur.

Les utilisateurs de la feuille de calcul se répartissent en deux groupes. Les développeurs construisent des modèles, les adaptent à divers usages,  traquent les bugs et réparent les failles. Les utilisateurs, eux, consultent les tableurs, font de la saisie, réordonnent lignes et colonnes, modifient les couleurs, les typographies et la mise en page des documents, réagencent les feuilles des classeurs. En parallèle, certains empruntent la voie médiane et se comportent comme des utilisateurs/développeurs en écrivant des macros (petites formules incantatoires dédiées à l’optimisation des performances d’évaluation des cellules). Or, c’est l’apparition de ces macro-commandes, en 1993 pour Excel, qui a permis au tableur en tant qu’outil de devenir un environnement de programmation à part entière. En somme, l’utilisation de ce type de logiciel intègre une grande diversité de tâches: nommer, lier, quantifier, classer, catégoriser, ordonner et programmer. En tant que cadre conceptuel, le tableur se présente donc comme un outil composite mettant à profit différentes fonctions cognitives selon l’utilisateur. 

Avec l'arrivée d'internet, la démocratisation des logiciels de mise en page et de retouche photographique ont achevé grâce aux sciences de l’ingénieur qui étaient au cœur du mouvement technocratique américain des années 1930 de transformer ce qui était à la base une grille de chiffres verts sur fond noir en une solution universelle, se proposant de gérer à peu près tout ce qui vit et survit sur cette planète et cela d'autant plus facilement que nous acceptions docilement de nous laisser pour ainsi dire "numériser". Nombreux sont ceux pour qui l'idée d'ouvrir un tableur évoque une sensation de malaise plutôt que de soulagement. L'apparition de cette grille noire sur fond blanc évoque en effet des analogies moins séduisantes que celles du trombone, de la poubelle, du glisser/déposer ou du copier-coller. Le tableur exige de son utilisateur de démembrer n’importe quel problème pour ranger chaque chose à sa place. La feuille de calcul reste cependant aujourd'hui, et malgré une obsolescence programmée, un outil caractéristique du milieu du développement logiciel. Référence de base de l’imaginaire informatique, couteau suisse bureaucratique, panacée universelle technocratique, à ce jour Excel règne encore dans toutes les administrations. Les évolutions actuelles sont principalement d'ordre cosmétique. Le travail de fond des ingénieurs ne vise plus à repousser les limitations techniques, mais plutôt à mettre choix des stratégies marketing. La compétition actuelle entre Google Sheets, Airtable, OnlyOffice ou encore Excel Online se joue principalement sur l’approche des fonctionnalités collaboratives en ligne et sur l’automatisation dite “intelligente” des tâches.

En effet, les innovations récentes dans le domaine des tableurs concernent principalement l'adaptation au travail en réseau, à savoir le partage de documents et une approche graduelle vers une automatisation de la saisie, de la correction et de la programmation des feuilles de calcul. L’auto-remplissage, d'auto-correction et l’automatisation de tâches initialement dévolues aux humains considérés comme faillibles et donc potentiellement porteurs d’inexactitudes voire d'erreurs est présentée comme une panacée. Toutefois, comme on l’a vu avec les erreurs liées aux acronymes des gènes humains, ces automatismes posent question. Si l’erreur est humaine, l’intelligence artificielle ne l’est pas et certains proposent donc de s’en remettre aux machines elles-mêmes pour saisir, traiter et enfin décider. Une histoire qui s’écrit dès les débuts de l’informatique.

Depuis les années 1980, la réflexion philosophique sur la nature de l’esprit a intégré un nombre croissant d’études provenant du champ des sciences cognitives. Ces dernières intègrent divers champs du savoir, comme la psychobiologie, la neuro-ergonomie et les sciences informatiques.  Du côté de la théorie de la connaissance, on remarque un intérêt particulier pour la philosophie de l’esprit, qui traite spécifiquement des représentations mentales, par exemple la théorie de l’esprit, qui désigne la faculté à se représenter les processus mentaux d’autrui. S’inscrivant dans la lignée de la psychologie, l’approche des neurosciences utilise les technologies de pointe et met en place des protocoles expérimentaux spécifiques visant à vérifier ou à falsifier des modèles précis des processus étudiés.  Dans ce contexte, il n’est pas rare que les résultats obtenus contredisent ce qui a pu être enseigné par le passé dans le champ de la psychologie. Ce renouvellement des modèles théoriques et de la structure de la connaissance entraîne en outre un glissement sémantique de la signification du terme “paradigme”.  
Tableurs et intelligence
Tel qu’il avait été employé au XXe siècle en philosophie des sciences par Thomas Kuhn, le concept de “paradigme” désignait le cadre explicatif global dans lequel s’inscrivent à la fois les connaissances scientifiques et le discours théorique.  Dans le cadre des sciences cognitives, un paradigme désigne le modèle explicatif et l’hypothèse de travail mis à l’épreuve par un groupe de chercheurs qu’il s’agit de falsifier ou de vérifier. À travers ce glissement, nombre d’objets d’étude traditionnellement unitaires en psychologie ont donc été fragmentés. On a dès lors plutôt affaire à des problèmes spécifiques, dont l’intégration dans des modèles plus abstraits n’est pas nécessairement adressée de prime abord. Et pourtant, dans leur diversité, les sciences cognitives semblent bien procéder d’un paradigme au sens de cadre de réflexion abstrait, à savoir la comparaison du cerveau humain et de l’ordinateur, esquissée dès le XXè siècle par Alan Turing, Norbert Wiener ou encore John Von Neumann. Les recherches d’Alan Turing partaient de l’analogie entre le fonctionnement de la pensée humaine et le calcul automatisé. Avant l’apparition d’ordinateurs, le “test de Turing” était un modèle abstrait du fonctionnement d’un calculateur automatisé dont la validité mathématique ne permettait pas une compréhension intuitive. Les théories de Turing passent dans l’imaginaire collectif à partir des années 1980, avec notamment l'apparition du Macintosh, ordinateur personnel grand public iconique à la fois des avancées technologiques et des nouveaux principes de design de l’époque. 

Du côté du marketing, on annonce au public l'apparition d'une informatique intuitive, à portée de tous, parlant en quelque sorte au corps plutôt qu’à l’esprit.  Les tableurs arrivent sur le marché à peu près au même moment: l’Apple II est commercialisé en 1979 et il popularise l’utilisation des tableurs sur l’ancêtre d’Excel, Visicalc. Contrairement aux autres produits d’Apple Computers, qui visent l’introduction de l’ordinateur personnel dans les foyers, Visicalc s'adresse directement aux corps des comptables. Ceux-ci acceptent immédiatement la remédiatisation de leur environnement de travail à travers l'analogie entre document numérique et feuille de papier.  s'enthousiasment à l'idée de pouvoir y insérer des formules "mathématiques" (que l'on nomme également fonctions ou macros). Ce nouvel outil permettra d'augmenter la puissance d'archivage, la vitesse de consultation et surtout la capacité de suivre les comptes d'une entreprise en temps "réel".  Le métier se transforme, il ne s'agit plus de poser des calculs, mais de mettre en forme et d'organiser la faisabilité et la lisibilité des calculs dont la machine est désormais responsable. La complexification des systèmes d'exploitation, comparables aux fondations d’un bâtiment en termes d'architecture, a permis d'introduire de nouvelles sophistications: couleur et typographie. L’étape suivante est celle de la localisation: les ordinateurs jonglent avec les fuseaux horaires, les alphabets non-latins et les monnaies internationales, s’adaptant progressivement à une clientèle globalisée.


De la cellules au cloud : une récursivité systémique 
Les données présentées par une feuille de calcul sont appelées cellules; elles sont rangées en lignes et colonnes. Le terme « cellule » utilisé ici dans le sens banal d’un élément du tableau, renvoie à l’idée que le modèle du tableur est l’organisme vivant, car le document voué à évoluer dans le temps en fonction de la mise à jour de ses données. Toutes ses cellules peuvent effectivement être modifiées en fonction de leurs relations les unes aux autres. Cette référence à une corporalité de la feuille de calcul met donc à profit l’imagerie de l’explication scientifique et l’étude du vivant. De plus, les données étant aujourd’hui collectées, elles sont aussi destinées à être rassemblées puis traitées par des algorithmes spécialisés dans la manipulation de grandes bases de données, parfois à l’insu des utilisateurs. Pour comprendre comment le sens commun associé à ces outils s’imbrique dans la réalité de l’informatique connectée, il s’agit donc de déterminer les limites de la référence métaphorique à la biologie. Ce qu’on appelle le cloud computing consiste à rassembler sur des serveurs gigantesques les données, mais aussi de ressources logicielles et de puissances de calcul astronomiques, afin que les utilisateurs puissent y accéder à distance. Le cloud, c’est aussi la possibilité pour les plus puissants acteurs de l’industrie technologique de garder la mainmise sur les données afin de les adapter aux usages, et ainsi de maximiser leur influence sur les comportements des utilisateurs. Par la construction de méta-données, c'est-à-dire de données qui décrivent d’autres données, les entreprises qui distribuent les programmes les plus utilisés montent en dimension. L’imbrication de données brutes dans des objets plus complexes vise explicitement l’intégration de ces derniers à des classes d’objets toujours plus abstraites. La récursivité est un concept qui désigne ce phénomène d’imbrication, présenté ici dans son expression la plus simple. Il faut souligner que ce type d’opération devient rapidement coûteux en termes de puissance de calcul dans le cas de calculs algorithmiques complexes sur de grandes bases de données.  Pour cette raison entre autres, appliquer ce type de techniques à grande échelle était impossible. Bien que la récursivité soit un principe relativement simple, son application massive à des bases de données toujours plus grandes a donc été une des conditions de possibilité du développement du traitement de données en extrêmement grande quantité, qu’on appelle couramment le Big Data.

2- Systèmes d’exploitation de l’intuition


[visuel 3 : tetris]

 Empathie et cybernétique
 	La naissance de l’informatique, qu’on associe généralement aux recherches d’Alan Turing sur les machines puis de celles de Norbert Wiener sur la cybernétique, procède d’une démarche qui a pour but de comprendre le fonctionnement de l’esprit humain en créant des automates à l’image de fonctions cognitives spécifiques.
Dans le champ des sciences cognitives, il existe une notion qui permet de rendre compte de certains processus d’imitation: les neurones miroirs. Découverts par Giacomo Rizzolatti à la fin des années 1990, ces neurones jouent un rôle de premier plan dans l’imitation des actions observées et dans l’apprentissage, car ils s’activent à la fois lorsque l’individu exécute une action et quand il l’observe. De plus, leur activation contribue au déchiffrage des expressions faciales et par extension à la perception des émotions d’autrui. La découverte de cette catégorie de neurones a donc renouvelé le discours scientifique sur les liens entre apprentissage et imitation. Il n’est dès lors pas étonnant que les multiples débats autour des neurones miroir aient remis en question certaines théories psychologiques concernant les relations entre théorie de l'esprit et de l'empathie cognitive, notamment dans le cadre de l’étude des troubles autistiques. La question de l’empathie, qui a refait surface dans le cas de l’étude du travail sur ordinateur et du télétravail, ne semble pas pertinente pour interroger le conditionnement psychologique et comportemental associé à l’utilisation d’outils génériques tels que les logiciels de traitement de texte et les tableurs. La question de l’imitation et de la communication non-verbale est pourtant importante pour l’étude des paradigmes en jeu dans l’ergonomie cognitive, discipline qui s’intéresse notamment au statut des processus mentaux comme la perception, la mémorisation, et l’attention dans l’étude du travail. Pour formuler le problème de façon récursive, on pourrait se demander quel est le rôle de l’empathie dans la conception des outils conceptuels qui permettent la création de solutions logicielles. En d’autres termes, si l’on considère les langages de programmation et autres cadres conceptuels permettant de développer des outils numériques comme des méta-outils, la question serait de demander à qui ils s’adressent, ou plutôt à quels types d’usages ces outils sont adaptés. Friedrich Kittler, pionnier des études médiatiques en Allemagne, s’est beaucoup intéressé au concept de feedback, concept qu’on peut traduire en français par le terme “rétroaction”. Du point de vue de l’archéologie des média, il s’agirait donc de chercher à comprendre comment la mise en place des langages informatiques dans la première moitié du XXe siècle a influencé le développement des pratiques dans le champ de la programmation; dans un deuxième temps, il s’agirait d’interroger les pratiques informatiques contemporaines afin de déterminer comment les représentations communiquées implicitement à travers les langages de programmation puis à travers les logiciels ont pu affecter l’imaginaire même des utilisateurs actuels. Or, il s’avère que les chercheurs qui posent ce type de question s’accordent à dire que les comportements des utilisateurs sont contraints par des représentations mentales spécifiques, basées sur des hypothèses de travail coûteuses en termes d’ergonomie cognitive: plus leurs tâches sont automatisées, plus les humains se trouvent enclins à l’erreur: ils “sortent de la boucle”. 

Ergonomie sélective, conditionnement
La prochaine question qui nous intéressera est celle de l’influence des feuilles de calcul sur l’imagination collective, et la question du primat de la finance sur le réel. Rappelons pour commencer que le terme anglophone “venture capital”, qui évoque à première vue la grande aventure du capitalisme, désigne les acteurs financiers qui investissent des fonds collectifs, parfois même publics, dans des projets à haut risque. Considérons l’exemple d’une entreprise qui fabrique des accessoires de bureau comme des trombones à papier. L’entreprise peut modéliser ses gains et déterminer s’il est plus profitable d’augmenter ou de diminuer le prix de vente de ses produits en fonction de la conjoncture et du cours en bourse de la matière première, en l'occurrence, l’acier. On pourrait supposer que leurs décisions n’aient que peu d’impact sur le comportement de leurs acheteurs. Les trombones sont des objets de bureautique de base: s’ils sont trop flexibles ou trop rigides, ils peuvent à force d’utilisation blesser les doigts de leurs utilisateurs. Dans ce cas, à moins d’avoir le monopole sur le marché mondial, la cote boursière de l’entreprise risquerait d’en être impactée. Par contraste, si une entreprise qui propose des logiciels de vidéosurveillance décide de limiter le nombre de paramètres accessibles à ses acheteurs, ou plutôt si une telle entreprise décidait d’inverser le principe du “glisser du doigt dans de gauche à droite” pour se distinguer de ses compétiteurs, il est probable que le cours de ses actions en bourse chuterait rapidement car les utilisateurs se représenteraient ce principe comme une interface contre-intuitive. La différence n’a rien d’anodin: dans le premier cas, les séquelles physiques sont manifestes, les conséquences financières sont directement liées à l’expérience des utilisateurs; on peut toutefois évaluer l’ergonomie de l’objet par rapport à une référence concrète qui est l’intégrité du corps physique de l’utilisateur. Dans le second cas cependant, le préjudice ergonomique n’est pas ancré dans une réalité empirique. Le conditionnement des utilisateurs d’interfaces numériques ne fait l’objet d’aucune régulation légale. Les états peinent à légiférer sur l’exploitation des données des utilisateurs par les fournisseurs de services en ligne, mais l’impact à long terme des modèles ergonomiques des systèmes d’interaction numériques ne font pas l’objet d’un débat public approfondi. Quelles seront donc les incidences à long terme des automatismes mis en place par les fabricants de programmes informatiques afin de fluidifier toujours plus l’expérience de leurs utilisateurs? Si l’on prend pour référence le type d’attention en jeu dans la lecture des Conditions Générales d’Utilisation des produits numériques, on peut imaginer avoir ici encore un point de départ pour réfléchir aux relations entre ergonomie cognitive et haute finance. La signature des CGU s’effectue par un simple clic, et elle donne instantanément accès aux services proposés sous l’apparence de gratuité. Bien que le public soit conscient que cette gratuité se paye par une contribution de ses données personnelles à l’économie du Big Data, il semble bien que ce soient les acteurs du numérique, ces nouveaux technocrates, qui déterminent les lois qui gouvernent la vie des internautes.

Réseaux et individus 
D’après le dictionnaire en ligne Larousse.fr, la technocratie se définit comme un “Système politique ou économique dans lequel les experts, techniciens et fonctionnaires supplantent, en fait ou en droit, les responsables politiques dans la prise des décisions (souvent péjoratif)”. Dans ce sens, l’histoire des logiciels informatiques grand public est donc un domaine de recherche privilégié pour observer la faillite graduelle de la gouvernance politique des états au profit d’un nouveau type de gouvernance sociétale. On pourrait dès lors dire que le gouvernement des vies individuelles sur le web est symptomatique la disparition de la démocratie évoquée par Rémi Jardat dans ses recherches sur les relations entre les GAFA et états. Organisés par les chefs des entreprises les plus importantes dans le domaine du numérique, eux-mêmes considérés comme des experts dans le champ de la technologie, le processus de prise de décision ne provoque de débat public qu’à propos d’épiphénomènes massivement médiatisés comme l’affaire Cambridge Analytica.  Les relations entre les tableurs et le marketing font l’objet d’études détaillées, et il en est de même de l’impact des tableurs sur les processus de management. Dans le contexte de l’adoption de l’intelligence artificielle pour un nombre croissant de problèmes, ce qui est en jeu est donc le passage de la gestion des tableurs par des agents humains au traitement automatisé des bases de données. En résumé, ce tournant historique peut être interprété comme la montée en puissance d’une nouvelle technocratie dépourvue de régulation par les sceptiques, ou comme l’annonce de la diminution graduelle de l’erreur humaine dans la manipulation de données par les enthousiastes.

La situation est analogue en ce qui concerne les machines administratives mises en place par les États contemporains: déclarations d’impôts, obtention de documents légaux, gestion d’entreprise, inscriptions aux universités, Pôle Emploi et Caisse des Allocations Familiales disposent d’interfaces officielles dédiées qui transforment plutôt qu’elles ne résolvent les problèmes logistiques auxquelles elles répondent. Aux formulaires papier se sont substitués les formulaires électroniques, dont l’accès sur des portails sécurisés n’est que le premier obstacle. Si le fonctionnement de l’état peut être analysé du point de vue de sa bureaucratie, la construction et la mise en circulation d’outils administratifs informatiques ne sont pas anodines. Ces plateformes sont le signe de la généralisation d’un phénomène de sous-traitance appliqué aux méthodes de gouvernance. Pour désigner les pratiques du politique à travers des tableurs et des formulaires, on pourrait alors parler de démocratie managériale. Les citoyens devenus utilisateurs sont priés de s’auto-gérer à l’aide d’outils souvent contre-intuitifs, dont l’utilisation présuppose un ensemble de pré-requis techniques. 

Du point de vue des méthodes de travail elles-mêmes, le maître-mot du management contemporain pourrait être l’évaluation par les pairs : plus besoin d’experts pour évaluer les compétences et examiner les conditions de travail des employés, leurs témoignages réunis sous forme de bases de données suffisent. Qu’il s’agisse d’administrateurs, d’enseignants, d’ouvriers ou de salariés du secteur tertiaire, tous sont soumis à des enquêtes d’opinion à partir desquelles leurs managers tireront leurs conclusions. Alors qu’il est nécessaire de questionner ces ces interfaces numériques, dont on sait qu’elles sont responsables d’un nouveau type de  pénibilité, le travail administratif est fragmenté et redistribué, ce qui aboutit à une augmentation de l’opacité des motifs de prise de décision. La bureaucratie numérique, en tant que dispositif de contrôle, a été étudiée depuis la fin des années 1970, mais les recherches récentes de penseurs comme Mark Fisher David Graeber et Alain Supiot signalent un renouveau dans la réflexion dans l’analyse de la dimension technocratique de la bureaucratie numérique du point de vue des rapports entre rentabilité et qualité de vie au travail. Les moyens de production des travailleurs ont désormais une double fonction: les outils de collaboration numériques sont aussi des instruments de contrôle du rendement.



[visuel 4 : = ÊTRE_MANGÉ_PAR( [Column - 1] )]


Conclusion

La feuille de calcul constitue une strate souterraine des technocraties contemporaines, s’imposant à l’intuition à travers un ensemble de variations allant du formulaire en ligne à la mythologie du Big Data. Permettant l'agencement de contenus hétérogènes au sein d'une structure extrêmement régulière, la feuille de calcul est le parangon discret de la quantification du réel. Malgré un certain nombre de critiques portées à son égard, la feuille de calcul gagne toujours plus de terrain en se déployant à travers toutes les instances administratives et ce à toutes les échelles, de l’individu à la multinationale en passant par l'État. Interpréter sa fonction dans la société contemporaine présuppose un ensemble de biais épistémologiques: le tableur est populaire, indispensable, il s’est imposé comme "couteau suisse des gestionnaires". Préexistant aux prises de décisions budgétaires et aux stratégies marketing, la grille d’interprétation imposée par la feuille de calcul a subjugué les imaginaires. Alors que les géants du numérique, seuls en position d'imposer une transformation des paradigmes de la bureaucratie numérique, se concentrent sur l’application de l’intelligence artificielle à des bases de données toujours plus grandes, le sens commun semble moins que jamais prêt à abandonner le système de représentation matriciel qui forme la pierre angulaire de la gouvernance par le numérique. 

Au cœur des enjeux du conditionnement de la pensée dont Excel est le nom, la question de la détermination de la perception même de la réalité est un problème saillant. Qui prend la responsabilité de légiférer et de proposer des critères permettant de cadrer les usages faits de ces technologies? La prochaine innovation en termes de comptabilité consistera-t-elle à décharger l'humain de la part de responsabilité en jeu dans le calcul du réel?

